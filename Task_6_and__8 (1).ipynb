{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyEzyRX59I03"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the checkpoint from before and train on the IMDB dataset"
      ],
      "metadata": {
        "id": "P95KvHXq21RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the best model from Task 5\n",
        "best_model = MLP(input_size=768)\n",
        "best_model.load_state_dict(torch.load(\"best_checkpoint.pt\"))\n",
        "best_model.to(device)\n",
        "\n",
        "# Define loss function & optimizer with lower learning rate\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "yax9T13f20-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have choosen 200 samples from imdb also for continuous learning"
      ],
      "metadata": {
        "id": "9EYKj2nptewx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load IMDB dataset\n",
        "imdb_df = pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")\n",
        "\n",
        "# Convert sentiment labels to binary (0 = negative, 1 = positive)\n",
        "imdb_df[\"label\"] = imdb_df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
        "imdb_df.drop(columns=[\"sentiment\"], inplace=True)\n",
        "\n",
        "# Select 200 random samples for training, validation, and test\n",
        "num_samples = 200\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    imdb_df[\"review\"], imdb_df[\"label\"], test_size=0.2, random_state=42, stratify=imdb_df[\"label\"]\n",
        ")\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.25, random_state=42, stratify=train_labels\n",
        ")\n",
        "\n",
        "# Take only the first 200 samples\n",
        "train_subset_texts, train_subset_labels = train_texts[:num_samples], train_labels[:num_samples]\n",
        "val_subset_texts, val_subset_labels = val_texts[:num_samples], val_labels[:num_samples]\n",
        "test_subset_texts, test_subset_labels = test_texts[:num_samples], test_labels[:num_samples]\n",
        "\n",
        "# Convert text to embeddings\n",
        "embedding_model = TextEmbeddings()\n",
        "\n",
        "X_train_imdb = torch.tensor(embedding_model.get_embedding(train_subset_texts.tolist()), dtype=torch.float32)\n",
        "X_val_imdb = torch.tensor(embedding_model.get_embedding(val_subset_texts.tolist()), dtype=torch.float32)\n",
        "X_test_imdb = torch.tensor(embedding_model.get_embedding(test_subset_texts.tolist()), dtype=torch.float32)\n",
        "\n",
        "y_train_imdb = torch.tensor(train_subset_labels.values, dtype=torch.long)\n",
        "y_val_imdb = torch.tensor(val_subset_labels.values, dtype=torch.long)\n",
        "y_test_imdb = torch.tensor(test_subset_labels.values, dtype=torch.long)\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "batch_size = 16\n",
        "train_loader_imdb = DataLoader(TensorDataset(X_train_imdb, y_train_imdb), batch_size=batch_size, shuffle=True)\n",
        "val_loader_imdb = DataLoader(TensorDataset(X_val_imdb, y_val_imdb), batch_size=batch_size, shuffle=False)\n",
        "test_loader_imdb = DataLoader(TensorDataset(X_test_imdb, y_test_imdb), batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "FxRwcRI5207d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OK51GJMK9hXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a separate TensorBoard writer for Task 6 logs\n",
        "writer_imdb = SummaryWriter(\"runs/IMDB_Training\")\n",
        "\n",
        "def continual_learning(model, train_loader, val_loader, checkpoint_path, dataset_name, num_epochs=5):\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        correct, total, train_loss = 0, 0, 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct, total, val_loss = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predicted = torch.argmax(outputs, dim=1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "\n",
        "        # Log Training & Validation Metrics to Separate TensorBoard\n",
        "        writer_imdb.add_scalar(f\"Loss/Train_{dataset_name}\", train_loss / len(train_loader), epoch)\n",
        "        writer_imdb.add_scalar(f\"Loss/Validation_{dataset_name}\", val_loss / len(val_loader), epoch)\n",
        "        writer_imdb.add_scalar(f\"Accuracy/Train_{dataset_name}\", train_acc, epoch)\n",
        "        writer_imdb.add_scalar(f\"Accuracy/Validation_{dataset_name}\", val_acc, epoch)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    writer_imdb.flush()\n",
        "    print(f\"\\nBest Validation Accuracy on {dataset_name}: {best_val_acc:.4f}\")\n",
        "    print(f\"Continual learning model saved at '{checkpoint_path}'\")\n",
        "\n",
        "# Train on IMDB dataset\n",
        "checkpoint_imdb = \"checkpoint_imdb.pt\"\n",
        "continual_learning(best_model, train_loader_imdb, val_loader_imdb, checkpoint_imdb, \"IMDB\", num_epochs=5)\n",
        "\n",
        "# Close the separate writer\n",
        "writer_imdb.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyWoQd3J204t",
        "outputId": "2df7492c-e5fe-4c82-858c-27a6eb17f5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 5/5\n",
            "\n",
            "Best Validation Accuracy on IMDB: 0.7400\n",
            "Continual learning model saved at 'checkpoint_imdb.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, dataset_name):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"\\nFinal Test Accuracy on {dataset_name}: {test_acc:.4f}\")\n",
        "\n",
        "    # Log final test accuracy in Separate TensorBoard\n",
        "    writer_imdb.add_scalar(f\"Accuracy/Test_{dataset_name}\", test_acc)\n",
        "    writer_imdb.flush()\n",
        "\n",
        "# Evaluate on IMDB\n",
        "evaluate_model(best_model, test_loader_imdb, \"IMDB\")\n",
        "\n",
        "# Close the writer after evaluation\n",
        "writer_imdb.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EJR1MQv202A",
        "outputId": "1ae10709-ad32-4288-f829-e67ada664cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy on IMDB: 0.7800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the validation loss and accuracy on the validation set of the Dataset 1 and IMDB dataset"
      ],
      "metadata": {
        "id": "Vxx1BIzH42V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load separate models for each dataset\n",
        "best_model_sst2 = MLP(input_size=10000)  # Model for SST2 (BoW)\n",
        "best_model_sst2.load_state_dict(torch.load(\"checkpoint_bow.pt\"))\n",
        "best_model_sst2.to(device)\n",
        "\n",
        "best_model_imdb = MLP(input_size=768)  # Model for IMDB (Embeddings)\n",
        "best_model_imdb.load_state_dict(torch.load(\"checkpoint_imdb.pt\"))\n",
        "best_model_imdb.to(device)\n",
        "\n",
        "# Create a separate TensorBoard writer for Task 7 logs\n",
        "writer_eval = SummaryWriter(\"runs/Evaluation\")\n"
      ],
      "metadata": {
        "id": "j9Ojvnrh20zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def log_confusion_matrix(model, val_loader, dataset_name):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", ax=ax)\n",
        "    ax.set_xlabel(\"Predicted Labels\")\n",
        "    ax.set_ylabel(\"True Labels\")\n",
        "    ax.set_title(f\"Confusion Matrix - {dataset_name}\")\n",
        "\n",
        "    # Log to TensorBoard (Separate from Training Logs)\n",
        "    writer_eval.add_figure(f\"Confusion Matrix/{dataset_name}\", fig)\n",
        "    writer_eval.flush()\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Confusion matrix logged for {dataset_name}\")\n"
      ],
      "metadata": {
        "id": "kvMV_uj220wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log Confusion Matrix for SST2 dataset (BoW Model)\n",
        "log_confusion_matrix(best_model_sst2, val_loader, \"SST2\")\n",
        "\n",
        "# Log Confusion Matrix for IMDB dataset (Embedding Model)\n",
        "log_confusion_matrix(best_model_imdb, val_loader_imdb, \"IMDB\")\n",
        "\n",
        "writer_eval.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEI596TX20t8",
        "outputId": "e69e588a-2211-4532-e857-2918be5402d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix logged for SST2\n",
            "Confusion matrix logged for IMDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def save_compressed_checkpoint(model, filepath):\n",
        "    \"\"\"Save the model in a compressed format.\"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "    torch.save(state_dict, filepath, _use_new_zipfile_serialization=True)\n",
        "    print(f\" Compressed checkpoint saved at: {filepath}\")\n",
        "\n",
        "# Apply compression in Task 5 (SST2 Checkpoint)\n",
        "save_compressed_checkpoint(best_model_sst2, \"compressed_checkpoint_sst2.pt\")\n",
        "\n",
        "# Apply compression in Task 6 (IMDB Checkpoint)\n",
        "save_compressed_checkpoint(best_model_imdb, \"compressed_checkpoint_imdb.pt\")\n",
        "\n",
        "# Verify file sizes\n",
        "print(f\"Compressed SST2 Checkpoint Size: {os.path.getsize('compressed_checkpoint_sst2.pt') / 1024:.2f} KB\")\n",
        "print(f\"Compressed IMDB Checkpoint Size: {os.path.getsize('compressed_checkpoint_imdb.pt') / 1024:.2f} KB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8HJdYylyoce",
        "outputId": "818cb6a7-d40c-4642-f0ce-fd551adfb25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Compressed checkpoint saved at: compressed_checkpoint_sst2.pt\n",
            " Compressed checkpoint saved at: compressed_checkpoint_imdb.pt\n",
            "Compressed SST2 Checkpoint Size: 20680.50 KB\n",
            "Compressed IMDB Checkpoint Size: 2216.50 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorboard"
      ],
      "metadata": {
        "id": "YAOoMk_n6LPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n"
      ],
      "metadata": {
        "id": "dnSBGgO220rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/Text_Classification\n"
      ],
      "metadata": {
        "id": "C-zyho_920pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/Training\n"
      ],
      "metadata": {
        "id": "bhqk3fjj20mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/IMDB_Training\n"
      ],
      "metadata": {
        "id": "cHMuKf856cpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/Evaluation\n"
      ],
      "metadata": {
        "id": "RcWw6e7j6pZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Load trained models from previous tasks\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "best_model_sst2 = MLP(input_size=10000)  # Model for SST2 (BoW)\n",
        "best_model_sst2.load_state_dict(torch.load(\"compressed_checkpoint_sst2.pt\"))\n",
        "best_model_sst2.to(device)\n",
        "\n",
        "best_model_imdb = MLP(input_size=768)  # Model for IMDB (Embeddings)\n",
        "best_model_imdb.load_state_dict(torch.load(\"compressed_checkpoint_imdb.pt\"))\n",
        "best_model_imdb.to(device)\n",
        "\n",
        "# Create a separate TensorBoard writer for final metrics\n",
        "writer = SummaryWriter(\"runs/Final_Metrics\")\n"
      ],
      "metadata": {
        "id": "uaDrA36z2dNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_final_loss_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct, total, total_loss = 0, 0, 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()  # Same loss function used in training\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "ka6g_GpC6A5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute final loss & accuracy for SST2 dataset\n",
        "final_train_loss_sst2, final_train_acc_sst2 = compute_final_loss_accuracy(best_model_sst2, train_loader)\n",
        "final_val_loss_sst2, final_val_acc_sst2 = compute_final_loss_accuracy(best_model_sst2, val_loader)\n",
        "final_test_loss_sst2, final_test_acc_sst2 = compute_final_loss_accuracy(best_model_sst2, test_loader)\n",
        "\n",
        "# Compute final loss & accuracy for IMDB dataset\n",
        "final_train_loss_imdb, final_train_acc_imdb = compute_final_loss_accuracy(best_model_imdb, train_loader_imdb)\n",
        "final_val_loss_imdb, final_val_acc_imdb = compute_final_loss_accuracy(best_model_imdb, val_loader_imdb)\n",
        "final_test_loss_imdb, final_test_acc_imdb = compute_final_loss_accuracy(best_model_imdb, test_loader_imdb)\n",
        "\n",
        "# Log Loss Metrics in TensorBoard\n",
        "writer.add_scalar(\"Final Loss/SST2 Train\", final_train_loss_sst2)\n",
        "writer.add_scalar(\"Final Loss/SST2 Validation\", final_val_loss_sst2)\n",
        "writer.add_scalar(\"Final Loss/SST2 Test\", final_test_loss_sst2)\n",
        "\n",
        "writer.add_scalar(\"Final Loss/IMDB Train\", final_train_loss_imdb)\n",
        "writer.add_scalar(\"Final Loss/IMDB Validation\", final_val_loss_imdb)\n",
        "writer.add_scalar(\"Final Loss/IMDB Test\", final_test_loss_imdb)\n",
        "\n",
        "# Log Accuracy Metrics in TensorBoard\n",
        "writer.add_scalar(\"Final Accuracy/SST2 Train\", final_train_acc_sst2)\n",
        "writer.add_scalar(\"Final Accuracy/SST2 Validation\", final_val_acc_sst2)\n",
        "writer.add_scalar(\"Final Accuracy/SST2 Test\", final_test_acc_sst2)\n",
        "\n",
        "writer.add_scalar(\"Final Accuracy/IMDB Train\", final_train_acc_imdb)\n",
        "writer.add_scalar(\"Final Accuracy/IMDB Validation\", final_val_acc_imdb)\n",
        "writer.add_scalar(\"Final Accuracy/IMDB Test\", final_test_acc_imdb)\n",
        "\n",
        "# Log Everything as Text in TensorBoard for Easy Screenshot\n",
        "final_metrics_text = f\"\"\"\n",
        "SST2 Dataset:\n",
        "- Final Train Loss: {final_train_loss_sst2:.4f} | Final Train Accuracy: {final_train_acc_sst2:.4f}\n",
        "- Final Validation Loss: {final_val_loss_sst2:.4f} | Final Validation Accuracy: {final_val_acc_sst2:.4f}\n",
        "- Final Test Loss: {final_test_loss_sst2:.4f} | Final Test Accuracy: {final_test_acc_sst2:.4f}\n",
        "\n",
        "IMDB Dataset:\n",
        "- Final Train Loss: {final_train_loss_imdb:.4f} | Final Train Accuracy: {final_train_acc_imdb:.4f}\n",
        "- Final Validation Loss: {final_val_loss_imdb:.4f} | Final Validation Accuracy: {final_val_acc_imdb:.4f}\n",
        "- Final Test Loss: {final_test_loss_imdb:.4f} | Final Test Accuracy: {final_test_acc_imdb:.4f}\n",
        "\"\"\"\n",
        "\n",
        "writer.add_text(\"Final Metrics\", final_metrics_text)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n",
        "print(\"Final loss and accuracy logged in TensorBoard without retraining.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vvzMdv96FzW",
        "outputId": "f9922ea5-ca9f-4994-fc3b-33db642dc907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss and accuracy logged in TensorBoard without retraining.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/Final_Metrics\n"
      ],
      "metadata": {
        "id": "dGCC1hyO6Jli"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}